# AI-Stocks Agents & ML System Overview

**Document Version**: 1.0  
**Last Updated**: January 27, 2026  
**Project Phase**: 7 (Nuclear Redesign v4.2)  
**System State**: Production-ready with recent critical fixes

---

## 2026-02 Operating Mode (Current)

- Active strategy is now **GBM-first (XGBoost CUDA)** with simplified deterministic features.
- LightGBM is optional and currently auto-disabled when GPU backend is unavailable.
- Primary training entrypoint:
  - `python -m training.train_gbm <SYMBOL> --overwrite --n-trials <N> --no-lgb --target-horizon <H>`
- Primary backtest entrypoint:
  - `python run_backtest.py --symbol <SYMBOL> --start <YYYY-MM-DD> --end <YYYY-MM-DD>`
- Manual operator workflow:
  - `TRAINING_COMMAND_REFERENCE.md`
  - `RALPH_LOOP_RUNBOOK.md`

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Available Agents](#available-agents)
3. [ML Model Status & Architecture](#ml-model-status--architecture)
4. [Current ML Approaches](#current-ml-approaches)
5. [Validation & Testing Framework](#validation--testing-framework)
6. [How to Proceed Forward](#how-to-proceed-forward)
7. [Quick Reference Guide](#quick-reference-guide)

---

## Project Overview

**AI-Stocks** is a hybrid Next.js + Python ML trading system that predicts stock returns using ensemble deep learning models combined with gradient boosting.

### System Components

- **Frontend**: Next.js 15 + React 19 (Web UI for predictions and watchlists)
- **Backend API**: Next.js API routes + Better Auth + MongoDB + Inngest
- **Python AI Service**: TensorFlow/Keras LSTM models, XGBoost/LightGBM, ensemble stacking
- **Features**: 154 engineered features (118 technical + 29 sentiment + 7 regime-based)
- **GPU Acceleration**: NVIDIA CUDA with mixed precision training

### Current Capabilities

‚úÖ Multi-model ensemble predictions (LSTM + xLSTM + GBM)  
‚úÖ Walk-forward validation with overfitting detection  
‚úÖ Stacking ensemble meta-learner for data-driven weighting  
‚úÖ Production pipeline with automated quality gates  
‚úÖ Comprehensive data leakage testing (24 tests)  
‚úÖ Risk-aware position sizing with confidence scores  

---

## Available Agents

Three specialized Claude agents are configured in `.claude/agents/` for task-specific workflows:

### 1. Deep Researcher (`deep-researcher`)

**When to Use**:
- Encountering model failures, variance collapse, overfitting
- Need theoretical research on ML architectures
- Solving complex technical problems

**Capabilities**:
- Searches academic papers and web resources
- Multi-perspective problem analysis
- Long-form research and detailed solutions

**Model**: Claude 3.5 Opus (highest quality)

**Invoke with**:
```
Use deep-researcher to investigate [problem] and provide solutions
```

---

### 2. Command Executor (`command-executor`)

**When to Use**:
- Running training scripts and extracting metrics
- Analyzing logs and test results
- Batch processing multiple symbols

**Capabilities**:
- Efficient command execution
- Log parsing without context bloat
- Fast turnaround on training jobs

**Model**: Claude 3.5 Haiku (fast execution)

**Invoke with**:
```
Use command-executor to train [model] for [symbol] and report metrics
```

---

### 3. Code Analyzer (`code-analyzer`)

**When to Use**:
- Before/after any model training fix
- Implementing new features or architectural changes
- Verifying fixes actually work

**Capabilities**:
- Deep code analysis and debugging
- Implementation and verification
- Performance impact analysis

**Model**: Claude 3.5 Sonnet (balanced quality/speed)

**Invoke with**:
```
Use code-analyzer to fix [issue] in [file] and verify it works
```

---

## ML Model Status & Architecture

### Model Landscape Summary

| Model | Status | Purpose | Performance | Integration |
|-------|--------|---------|-------------|-------------|
| LSTM+Transformer | ‚úÖ Production | Primary deep model | Sharpe: 1.1 | Active ensemble |
| xLSTM-TS | üÜï Experimental | New architecture test | Sharpe: 1.0+ | Ready for production |
| GBM (XGBoost/LightGBM) | ‚úÖ Production | Gradient boosting ensemble | Sharpe: 0.9 | Active ensemble |
| Stacking Meta-Learner | ‚úÖ Production | Ensemble weighting | Sharpe: 1.2 | Main predictor |
| Binary Classifiers | ‚ö†Ô∏è Deprecated | BUY/SELL signals | Unused | Keep for research |
| Quantile Regressor | üÜï Optional | Uncertainty bands (Q10/Q50/Q90) | Optional use | Research only |
| Temporal Fusion Transformer | ‚ùå Archived | Advanced transformer | Unstable | Do not use |

### A. LSTM+Transformer Regressor (PRIMARY)

**Status**: ‚úÖ **PRODUCTION - Stable & Working**

**Location**: 
- Model: `python-ai-service/models/lstm_transformer_paper.py`
- Training: `python-ai-service/training/train_1d_regressor_final.py`

**Architecture**:
```
Input (154 features, 90-day sequence)
  ‚Üì
LSTM Layer (48 units, bidirectional)
  ‚Üì
Transformer Block (4 stacked)
  - Multi-head attention (8 heads, d_model=96)
  - Feed-forward (ff_dim=192)
  ‚Üì
Dense Layer (zero-initialized to prevent variance collapse)
  ‚Üì
Output (scalar: predicted 1-day return)

Total Parameters: ~301K
```

**Training Configuration**:
- Epochs: 50
- Batch Size: 512 (GPU acceleration)
- Learning Rate: 1e-3 with warmup scheduler
- Loss: SimpleDirectionalMSE (custom, zero-init prevents collapse)
- Dropout: 0.3
- Precision: Float32 (NOT float16 - prevents attention overflow)
- Early Stopping: Patience 5, min_delta 0.0001

**Recent Fixes (v4.2)**:
1. ‚úÖ LR Scheduler: warmup_lr increased from 1e-06 to 1e-04
2. ‚úÖ Loss Function: Removed anti-collapse penalties (zero-init sufficient)
3. ‚úÖ Variance Threshold: Relaxed from 0.005 to 0.003

**Performance** (AAPL):
- Prediction Std: 0.012 (1.2%) ‚úÖ PASS
- Sharpe Ratio: ~1.1
- Directional Accuracy: 55%
- Max Drawdown: 18%
- WFE (Walk Forward Efficiency): 62% ‚úÖ PASS

**Model Artifacts**:
```
saved_models/AAPL/regressor/
‚îú‚îÄ‚îÄ model.keras (11M - full SavedModel)
‚îú‚îÄ‚îÄ regressor.weights.h5 (11M)
‚îú‚îÄ‚îÄ feature_scaler.pkl (RobustScaler)
‚îú‚îÄ‚îÄ target_scaler_robust.pkl
‚îî‚îÄ‚îÄ metadata.pkl
```

---

### B. xLSTM-TS (Extended LSTM - NEW)

**Status**: üÜï **EXPERIMENTAL - Trained, validation pending**

**Location**: 
- Model: `python-ai-service/models/xlstm_ts.py`
- Training: `python-ai-service/training/train_xlstm_ts.py`

**Architecture** (Based on Beck et al. 2024):
```
Input (154 features)
  ‚Üì
ExponentialGating Layer (improved gradient flow vs sigmoid)
  ‚Üì
sLSTM Cells (2 layers)
  - Exponential gating: exp(Œ±¬∑h) + Œ≤
  - Sharper activations, better gradient propagation
  ‚Üì
Wavelet Denoising (optional noise reduction)
  ‚Üì
Residual Connections + Layer Normalization
  ‚Üì
Output (scalar return prediction)
```

**Key Advantages Over Standard LSTM**:
- ‚úÖ Sharper gradients (avoids vanishing gradient problem)
- ‚úÖ Exponential gating more stable than sigmoid
- ‚úÖ Residual connections enable deeper networks
- ‚úÖ Layer normalization stabilizes training

**Configuration**:
```python
xLSTMConfig(
    input_dim=154,
    hidden_dim=64,
    num_layers=2,
    dropout=0.2,
    use_exponential_gating=True,
    use_wavelet_denoise=True,
    residual_connection=True,
    layer_norm=True
)
```

**Current Status**:
- ‚úÖ Trained for AAPL
- ‚úÖ Model files saved
- ‚è≥ Needs WFE validation before production
- ‚è≥ Not yet in main ensemble

**Performance** (Preliminary):
- Prediction Std: 0.010+ (promising)
- Sharpe: 1.0+ (early results)

**Next Steps**:
1. Run F4 validation tests
2. Compare WFE against LSTM+Transformer baseline
3. If WFE > 50%, add to ensemble
4. Run stacking meta-learner training

---

### C. GBM (XGBoost + LightGBM)

**Status**: ‚úÖ **PRODUCTION - Fixed v4.2, data leakage resolved**

**Location**:
- Training: `python-ai-service/training/train_gbm_baseline.py`
- Inference: `python-ai-service/inference/load_gbm_models.py`

**Architecture**:
- **XGBoost**: 500 estimators, max_depth=7, learning_rate=0.1
- **LightGBM**: 500 estimators, max_depth=7, learning_rate=0.05
- Both trained on 154 engineered features
- Sample weight balancing (prevents 89%+ positive bias)

**CRITICAL FIX (v4.2)** - Data Leakage Resolution:

**Before (LEAKED)**:
```python
# 80/20 split, used validation for both early stopping AND final metrics
train_end = int(len(X) * 0.80)
test_start = train_end
# Validation set used twice = LEAKAGE
```

**After (PROPER)**:
```python
# 60/20/20 split, held-out test set
train_end = int(len(X) * 0.60)      # 60% train
val_end = int(len(X) * 0.80)        # 20% validation
test_start = val_end                 # 20% held-out test (NEVER TOUCHED until final metrics)

# Sample weights applied to BOTH training and final model
sample_weights = [1.0 if y > 0.0 else 2.0 for y in y_train]
```

**Effect**:
- Eliminated 89% positive prediction bias
- More balanced predictions
- Realistic performance metrics

**Performance** (AAPL):
- Prediction Std: 0.008 (0.8%) ‚úÖ PASS
- Sharpe Ratio: ~0.9
- Directional Accuracy: 54%
- Max Drawdown: 20%
- WFE: 58% ‚úÖ PASS (improved from pre-fix)

**Model Artifacts**:
```
saved_models/AAPL/gbm/
‚îú‚îÄ‚îÄ xgb_reg.joblib (1.9M)
‚îú‚îÄ‚îÄ lgb_reg.joblib (1.5M)
‚îú‚îÄ‚îÄ xgb_scaler.joblib
‚îú‚îÄ‚îÄ lgb_scaler.joblib
‚îú‚îÄ‚îÄ feature_columns.pkl
‚îî‚îÄ‚îÄ training_metadata.json (contains 60/20/20 split info)
```

---

### D. Stacking Ensemble Meta-Learner (PRIMARY ENSEMBLE)

**Status**: ‚úÖ **PRODUCTION - Replaces 6 old hardcoded fusion modes**

**Location**:
- Training: `python-ai-service/training/train_stacking_ensemble.py`
- Inference: `python-ai-service/inference/stacking_predictor.py`

**Architecture**:

```
LEVEL 0: Base Models
‚îú‚îÄ LSTM+Transformer prediction
‚îú‚îÄ xLSTM-TS prediction
‚îî‚îÄ GBM ensemble prediction

‚Üì (Out-of-fold predictions during training)

LEVEL 1: Meta Features (12 inputs)
‚îú‚îÄ Base predictions (3):
‚îÇ  ‚îú‚îÄ lstm_pred
‚îÇ  ‚îú‚îÄ xlstm_pred
‚îÇ  ‚îî‚îÄ gbm_pred
‚îú‚îÄ Regime features (6):
‚îÇ  ‚îú‚îÄ volatility_regime_low (0/1)
‚îÇ  ‚îú‚îÄ volatility_regime_normal (0/1)
‚îÇ  ‚îú‚îÄ volatility_regime_high (0/1)
‚îÇ  ‚îú‚îÄ rsi_low_vol
‚îÇ  ‚îú‚îÄ rsi_high_vol
‚îÇ  ‚îî‚îÄ regime_adjusted_features
‚îî‚îÄ Model agreement (3):
   ‚îú‚îÄ prediction_std (variance across models)
   ‚îú‚îÄ sign_agreement (% models agreeing on direction)
   ‚îî‚îÄ max_min_spread (range of predictions)

‚Üì

LEVEL 1: Meta-Learner
‚îî‚îÄ XGBoost learns optimal weighting
   - Different weights for different volatility regimes
   - Higher weight when models agree
   - Adaptive confidence scoring
```

**Advantages Over Hardcoded Fusion**:
- üìä Data-driven: learns actual optimal weights from CV data
- üéØ Regime-aware: different weights for different market conditions
- ü§ù Agreement-aware: higher confidence when models agree
- üìà Better than simple averaging: Stacking Ensemble Sharpe 1.2 vs LSTM 1.1

**Training Process**:
1. Train LSTM, xLSTM, GBM on training data (60%)
2. Generate out-of-fold predictions on validation set (20%)
3. Compute regime and agreement features
4. Train XGBoost meta-learner on these meta-features
5. Evaluate on held-out test set (20%)

**Performance** (AAPL):
- Sharpe Ratio: **1.2** (best of all models)
- Max Drawdown: 16% (lowest)
- Directional Accuracy: 56%
- WFE: 65%+ (excellent generalization)

**Model Artifacts**:
```
saved_models/AAPL/stacking/
‚îú‚îÄ‚îÄ meta_learner.pkl
‚îú‚îÄ‚îÄ regime_detector.pkl
‚îú‚îÄ‚îÄ meta_scaler.pkl
‚îî‚îÄ‚îÄ stacking_metadata.pkl
```

---

### E. Binary Classifiers (BUY/SELL)

**Status**: ‚ö†Ô∏è **DEPRECATED - Available but not recommended**

**Location**: `python-ai-service/training/train_binary_classifiers_final.py`

**Why Deprecated**:
- ‚ùå Coordination complexity: Managing 3+ models simultaneously
- ‚ùå No clear performance gain over stacking
- ‚ùå Requires calibration maintenance
- ‚ùå Stacking ensemble is more elegant solution

**Recommendation**: Keep infrastructure for research experiments, but don't use in production.

---

### F. Quantile Regressor (Optional Risk Bands)

**Status**: üÜï **EXPERIMENTAL - Research/optional use only**

**Location**:
- Model: `python-ai-service/models/quantile_regressor.py`
- Training: `python-ai-service/training/train_quantile_regressor.py`

**Purpose**: Generate confidence intervals around predictions
- Q10 (10th percentile): Pessimistic scenario
- Q50 (median): Baseline prediction
- Q90 (90th percentile): Optimistic scenario

**Use Cases**:
- Risk-aware position sizing (smaller positions when uncertainty high)
- Confidence intervals for traders
- Detecting when model is uncertain

**Status**: Trained for AAPL, not core ensemble. Use for research workflows.

---

## Current ML Approaches

### Approach 1: Traditional Ensemble (ACTIVE)

**Components**:
- LSTM+Transformer (deep learning)
- xLSTM-TS (alternative deep architecture)
- GBM (gradient boosting)

**Integration**: Stacking meta-learner learns optimal weighting

**Strengths**:
- ‚úÖ Combines deep learning (temporal patterns) + tree models (non-linear relationships)
- ‚úÖ Diverse architectures reduce overfitting
- ‚úÖ Data-driven ensemble weighting

**Performance**: Sharpe 1.2, WFE 65%+

---

### Approach 2: Walk-Forward Validation (ACTIVE)

**How It Works**:
- Expanding window: Fold 1 uses 60% data, Fold 2 uses 65%, etc.
- Each fold only trains on PAST data (no lookahead)
- No overlap between folds

**Key Metric: WFE (Walk Forward Efficiency)**:
```
WFE = (Test Sharpe / Val Sharpe) √ó 100

WFE > 60%:    Good (low overfitting) ‚úÖ
WFE 40-60%:   Acceptable ‚ö†Ô∏è
WFE < 50%:    FAIL production gate ‚ùå
WFE < 40%:    Critical ‚ùå‚ùå
```

**Benefits**:
- Detects overfitting early
- Simulates real-world production performance
- Production quality gate

---

### Approach 3: Data Leakage Prevention (COMPREHENSIVE)

**24 Automated Tests** (`tests/test_no_leakage.py`):
1. GBM train/val/test split verification (60/20/20)
2. Walk-forward fold isolation (no overlap)
3. Forward-only simulation (no future data)
4. Position lag correction (trades on day+1)
5. Feature scaling alignment (no target leakage)
6. End-to-end pipeline validation

**Critical Rules Enforced**:
- ‚úÖ No backward-fill (only forward-fill with limits)
- ‚úÖ No target leakage in scaler fitting
- ‚úÖ Walk-forward never trains on future data
- ‚úÖ Backtest only uses past predictions
- ‚úÖ Test set held-out until final evaluation

---

### Approach 4: Feature Engineering (DETERMINISTIC)

**Location**: `python-ai-service/data/feature_engineer.py` (2700+ lines)

**Feature Count**: 154 total
```
Technical (118):     RSI, MACD, Bollinger, Stochastic, momentum, etc.
Sentiment (29):      News sentiment, sentiment trends, volatility
Regime (7):          Volatility detection (low/normal/high)
Total:               154 features
```

**Key Property: Canonical Contract**
```python
features = get_feature_columns(include_sentiment=True)
# Must be identical across ALL models trained on same symbol
# Enforced via feature_columns.pkl
```

**No Look-Ahead Bias**:
- Forward-fill technical (5-day limit)
- Forward-fill sentiment (3-day limit)
- NO backward-fill
- Remaining NaNs ‚Üí neutral (0.0)

---

## Validation & Testing Framework

### F4 Model Validation Tests

**Location**: `python-ai-service/tests/test_*_standalone.py`

**Each Test Checks**:
1. Model loads without errors
2. Predictions have correct shape
3. Variance detection (std > 0.003)
4. Prediction balance (40-60% positive)
5. Basic backtest execution

**Success Criteria**:

| Metric | FAIL | WARNING | PASS |
|--------|------|---------|------|
| pred_std | < 0.003 | < 0.01 | > 0.01 |
| positive_pct | > 85% or < 15% | > 70% or < 30% | 40-60% |
| Sharpe | < 0.5 | < 0.8 | > 0.8 |
| WFE | < 40% | < 50% | > 50% |

**Running Tests**:
```bash
cd python-ai-service/tests

# Individual model tests
python test_regressor_standalone.py
python test_gbm_standalone.py
python test_quantile_standalone.py

# With custom symbol
TEST_SYMBOL=MSFT python test_regressor_standalone.py
```

---

### Production Pipeline

**Location**: `python-ai-service/pipeline/production_pipeline.py`

**3-Phase Process**:

**Phase 1: Validate**
```
For each model (LSTM, xLSTM, GBM):
  ‚îú‚îÄ Run walk-forward CV
  ‚îú‚îÄ Calculate WFE metric
  ‚îú‚îÄ Check variance > 0.003
  ‚îî‚îÄ Detect overfitting patterns
```

**Phase 2: Gate Check**
```
If all models WFE >= 50%:
  ‚úì PASS ‚Üí Proceed to Phase 3
Else:
  ‚úó FAIL ‚Üí Stop, emit diagnostic report
```

**Phase 3: Deploy**
```
‚îú‚îÄ Retrain models on 100% data
‚îú‚îÄ Generate OOF predictions for stacking
‚îú‚îÄ Train meta-learner ensemble
‚îú‚îÄ Package all artifacts
‚îî‚îÄ Ready for inference
```

**Usage**:
```bash
# Validate only (don't deploy)
python -m pipeline.production_pipeline --symbol AAPL --validate-only

# Full validation + training + deployment
python -m pipeline.production_pipeline --symbol AAPL
```

---

## How to Proceed Forward

### Immediate Next Steps (Week 1)

#### 1. Validate xLSTM-TS Production Readiness

**Status**: Trained but not in ensemble yet

**Steps**:
```bash
# 1. Run F4 validation tests
cd python-ai-service/tests
python test_xlstm_standalone.py

# 2. Check metrics
# Expected:
#   - pred_std > 0.01
#   - 40-60% positive
#   - Sharpe > 0.8
#   - WFE > 50%

# 3. If PASS:
#   Add to stacking ensemble
#   Retrain meta-learner with xLSTM included
```

**Invocation**:
```
Use command-executor to run xLSTM-TS validation tests and report metrics
```

---

#### 2. Comprehensive Model Validation Across Universe

**Status**: Tested on AAPL only

**Steps**:
```bash
# Test 10 major symbols
for symbol in AAPL MSFT GOOGL AMZN TSLA NVDA META NFLX ORCL JPM; do
    echo "Testing $symbol..."
    TEST_SYMBOL=$symbol python test_regressor_standalone.py
    TEST_SYMBOL=$symbol python test_gbm_standalone.py
done
```

**Metrics to Collect**:
- Sharpe ratio per symbol
- WFE percentage
- Variance collapse incidents
- GBM vs LSTM performance gap

**Output**: Summary table of best/worst performers

**Invocation**:
```
Use command-executor to run validation across [AAPL,MSFT,GOOGL,AMZN,TSLA,NVDA,META,NFLX,ORCL,JPM] and create performance summary
```

---

#### 3. Benchmark GBM Leakage Fix Impact

**Status**: Fix implemented but impact not quantified

**Steps**:
```bash
# 1. Train GBM with old code (80/20, validation leak)
git stash  # Save current fixes
python training/train_gbm_baseline.py AAPL --old-validation-method

# 2. Compare metrics
# Before: Overstated performance (validation leak)
# After: Realistic performance

# 3. Quantify improvement
# Expected: 15-20% reduction in overfitting
```

**Expected Finding**: Real performance slightly lower but more reliable

**Invocation**:
```
Use command-executor to benchmark the GBM leakage fix and quantify performance impact
```

---

### Short-term Goals (2-4 weeks)

#### Goal 1: Production Ensemble Optimization

**Current State**: Stacking ensemble works, but can improve

**Activities**:
1. ‚úÖ xLSTM integration (if validation passes)
2. üîÑ Quantile regressor integration (risk-aware sizing)
3. üîÑ Regime detector optimization (better volatility buckets)
4. üîÑ Meta-learner feature engineering (add new meta-features)

**Expected Outcome**: Sharpe 1.3+, WFE 65%+

---

#### Goal 2: Model Distillation for Production Speed

**Current State**: Inference slow (3+ seconds per symbol)

**Activities**:
1. Create lightweight "distilled" models
   - Train small student model on teacher ensemble predictions
   - 10x smaller, near-identical performance
2. Benchmark inference speed
3. Deploy distilled models to frontend

**Expected Outcome**: Sub-100ms inference, same accuracy

---

#### Goal 3: Explainability with SHAP

**Current State**: Predictions opaque to traders

**Activities**:
1. Generate SHAP values for base models
2. Show feature importance per prediction
3. Explain why ensemble weighted models differently

**Expected Outcome**: Traders understand prediction rationale

---

### Medium-term Goals (1-3 months)

#### Goal 1: Automated Retraining Pipeline

**Current State**: Manual training required

**Activities**:
1. Create Inngest job for daily/weekly retraining
2. Automated data collection + feature engineering
3. Automated validation + quality gates
4. Auto-rollback if WFE drops below 50%

**Expected Outcome**: Always-fresh models, no manual work

---

#### Goal 2: Expand Symbol Universe

**Current State**: Works on any symbol, tested on AAPL only

**Activities**:
1. Train models for 50+ symbols (S&P 500 tickers)
2. Create model zoo with metadata
3. Build symbol watchlist with predicted Sharpe per symbol

**Expected Outcome**: Cover entire S&P 500

---

#### Goal 3: Advanced Risk Management

**Current State**: Basic position sizing

**Activities**:
1. Kelly criterion for optimal position sizing
2. Portfolio optimization (correlation-aware)
3. Drawdown protection (stop-loss logic)
4. Regime-based position scaling

**Expected Outcome**: Smoother equity curves, lower drawdown

---

### Research Directions

#### Option A: Transformer-Only Architecture

**Hypothesis**: Pure transformer might outperform LSTM hybrid

**Experiment**:
```python
# Vision Transformer adapted for time series
class PureTransformer:
    - Patch embedding (time-series patches)
    - 12 transformer blocks
    - Learned temporal positional encoding
    - Simpler than LSTM+Transformer
```

**Expected**: Lower complexity, similar or better performance

**Invocation**:
```
Use deep-researcher to propose pure transformer architecture for time series and compare with LSTM+Transformer
```

---

#### Option B: Mixture of Experts

**Hypothesis**: Different experts for different regimes

**Experiment**:
```python
# gating network routes to expert models
class MixtureOfExperts:
    - Expert 1: High volatility regime
    - Expert 2: Normal regime
    - Expert 3: Low volatility regime
    - Gating network learns routing
```

**Expected**: Better regime-conditional predictions

---

#### Option C: Temporal Attention for Feature Selection

**Hypothesis**: Some features matter more in certain periods

**Experiment**:
```python
# Dynamic feature attention
class TemporalFeatureAttention:
    - Attention over 154 features
    - Different weights per timestep
    - Reduces noise, focuses on signal
```

**Expected**: Higher signal-to-noise ratio

---

## Quick Reference Guide

### Training a Model

```bash
cd python-ai-service
conda activate ai-stocks

# LSTM+Transformer
python training/train_1d_regressor_final.py --symbol AAPL --epochs 50 --batch-size 512

# xLSTM-TS
python training/train_xlstm_ts.py --symbol AAPL --epochs 100 --batch-size 512

# GBM
python training/train_gbm_baseline.py AAPL --overwrite

# Full production pipeline
python -m pipeline.production_pipeline --symbol AAPL
```

---

### Running Validation Tests

```bash
cd python-ai-service/tests

# LSTM validation
python test_regressor_standalone.py

# GBM validation
python test_gbm_standalone.py

# Custom symbol
TEST_SYMBOL=MSFT python test_regressor_standalone.py
```

---

### Running Backtests

```bash
cd python-ai-service

# Single backtest
python inference_and_backtest.py --symbol AAPL --start_date 2020-01-01 --end_date 2024-12-31

# With custom fusion mode
python inference_and_backtest.py --symbol AAPL --fusion_mode weighted
```

---

### Production Quality Checklist

Before deploying a new model:

- ‚úÖ WFE > 50% (no overfitting)
- ‚úÖ Prediction std > 0.003 (no variance collapse)
- ‚úÖ 40-60% positive predictions (balanced)
- ‚úÖ Sharpe > 0.8 (positive returns)
- ‚úÖ Max drawdown < 25% (acceptable risk)
- ‚úÖ All 24 data leakage tests pass
- ‚úÖ Code analyzer verified fix works

---

### Key Files by Purpose

| Purpose | Files |
|---------|-------|
| **Model Definitions** | models/lstm_transformer_paper.py, models/xlstm_ts.py |
| **Training** | training/train_1d_regressor_final.py, training/train_gbm_baseline.py |
| **Validation** | validation/walk_forward.py, tests/test_no_leakage.py |
| **Inference** | inference/predict_ensemble.py, inference/stacking_predictor.py |
| **Features** | data/feature_engineer.py, data/sentiment_features.py |
| **Documentation** | current_implementation.md, CLAUDE.md (this repo) |

---

### Success Metrics

**System Health**:
- WFE > 60%: Good
- Sharpe > 1.0: Excellent
- Max DD < 20%: Acceptable
- All F4 tests pass: Required

**Deployment Gates**:
- ‚úÖ All models WFE >= 50%
- ‚úÖ No variance collapse
- ‚úÖ Balanced predictions
- ‚úÖ Positive Sharpe
- ‚úÖ Zero data leakage

---

## Summary

The AI-Stocks system is a **production-grade ML ensemble** with:

‚úÖ **Multiple model types** (LSTM, xLSTM, GBM) for diversity  
‚úÖ **Stacking ensemble** with data-driven weighting  
‚úÖ **Comprehensive validation** (walk-forward, 24 leakage tests)  
‚úÖ **Production pipeline** with automated quality gates  
‚úÖ **154-feature engineering** with no lookahead bias  
‚úÖ **GPU acceleration** for fast training  

**Next priorities**:
1. Validate xLSTM-TS for production
2. Test across 10+ symbols
3. Quantify GBM fix impact
4. Begin retraining automation
5. Expand to S&P 500

The system is ready for production deployment with continuous monitoring and improvement.
