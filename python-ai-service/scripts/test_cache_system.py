"""
Test script for data caching system validation.

Tests:
1. Cache creation and persistence
2. Cache invalidation
3. Feature count consistency
4. Index alignment validation
5. Training with cached data
6. Inference with cached data

Usage:
    python scripts/test_cache_system.py --symbol AAPL
"""

import sys
from pathlib import Path
import logging

# Add python-ai-service to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from data.cache_manager import DataCacheManager
try:
    # training entry for TFT (may accept different args in your project)
    from training.train_with_tft import train_with_tft
except Exception:
    train_with_tft = None
from inference_and_backtest import main as inference_main
import argparse

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_cache_creation(symbol: str):
    """Test 1: Cache creation and persistence."""
    logger.info("\n" + "="*70)
    logger.info("TEST 1: Cache Creation and Persistence")
    logger.info("="*70)
    
    cache_manager = DataCacheManager()
    
    # Clear existing cache
    try:
        cache_manager.clear_cache(symbol)
        logger.info(f"âœ“ Cleared existing cache for {symbol}")
    except Exception:
        logger.info("No existing cache to clear (ok)")
    
    # First fetch (should create cache)
    logger.info("Fetching data (should create cache)...")
    raw_df1, eng_df1, prep_df1, feat_cols1 = cache_manager.get_or_fetch_data(
        symbol=symbol,
        include_sentiment=True
    )
    
    # Check cache info
    cache_info1 = cache_manager.get_cache_info(symbol)
    assert cache_info1 is not None, "Cache info should exist after first fetch"
    logger.info(f"âœ“ Cache created: {cache_info1.get('prepared_shape', prep_df1.shape)}")
    
    # Second fetch (should load from cache)
    logger.info("Fetching data again (should load from cache)...")
    raw_df2, eng_df2, prep_df2, feat_cols2 = cache_manager.get_or_fetch_data(
        symbol=symbol,
        include_sentiment=True
    )
    
    # Verify data is identical-ish (shape and feature list)
    assert prep_df1.shape == prep_df2.shape, "Cached data shape mismatch"
    assert feat_cols1 == feat_cols2, "Feature columns mismatch"
    logger.info("âœ“ Cache loaded successfully, data matches")
    
    return True


def test_cache_invalidation(symbol: str):
    """Test 2: Cache invalidation."""
    logger.info("\n" + "="*70)
    logger.info("TEST 2: Cache Invalidation")
    logger.info("="*70)
    
    cache_manager = DataCacheManager()
    
    # Get cache info
    info_before = cache_manager.get_cache_info(symbol)
    if info_before:
        logger.info(f"Cache age: {info_before.get('age_hours', 0):.2f} hours")
    else:
        logger.info("No cache present before refresh")
    
    # Force refresh
    logger.info("Forcing cache refresh...")
    raw_df, eng_df, prep_df, feat_cols = cache_manager.get_or_fetch_data(
        symbol=symbol,
        include_sentiment=True,
        force_refresh=True
    )
    
    info_after = cache_manager.get_cache_info(symbol)
    logger.info(f"New cache age: {info_after.get('age_hours', 0):.2f} hours")
    
    assert info_after is not None and info_after.get('age_hours', 99) < 0.1, "Cache should be fresh after refresh"
    logger.info("âœ“ Cache refresh successful")
    
    return True


def test_feature_consistency(symbol: str):
    """Test 3: Feature count consistency across steps."""
    logger.info("\n" + "="*70)
    logger.info("TEST 3: Feature Count Consistency")
    logger.info("="*70)
    
    cache_manager = DataCacheManager()
    raw_df, eng_df, prep_df, feat_cols = cache_manager.get_or_fetch_data(
        symbol=symbol,
        include_sentiment=True
    )
    
    logger.info(f"Feature count: {len(feat_cols)}")
    logger.info(f"Prepared data shape: {prep_df.shape}")
    
    # Verify feature count
    cache_info = cache_manager.get_cache_info(symbol)
    assert cache_info is not None, "Cache info missing"
    assert cache_info.get('feature_count') == len(feat_cols), "Feature count mismatch in metadata"
    
    # Verify all features are in prepared_df
    missing_features = [f for f in feat_cols if f not in prep_df.columns]
    assert len(missing_features) == 0, f"Missing features in prepared data: {missing_features}"
    
    logger.info("âœ“ Feature consistency validated")
    
    return True


def test_training_with_cache(symbol: str):
    """Test 4: Training with cached data."""
    logger.info("\n" + "="*70)
    logger.info("TEST 4: Training with Cached Data")
    logger.info("="*70)
    
    if train_with_tft is None:
        logger.warning("train_with_tft not importable in this environment; skipping training test")
        return True
    try:
        results = train_with_tft(
            symbol=symbol,
            use_tft=False,  # Skip TFT for faster testing
            force_refresh=False,  # Use cache
            skip_cache=False
        )
        
        assert isinstance(results, dict), "Expected dict from train_with_tft"
        logger.info("âœ“ Training with cache completed (result dict returned)")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Training failed: {e}")
        return False


def test_inference_with_cache(symbol: str):
    """Test 5: Inference with cached data."""
    logger.info("\n" + "="*70)
    logger.info("TEST 5: Inference with Cached Data")
    logger.info("="*70)
    
    try:
        result = inference_main(
            symbol=symbol,
            backtest_days=30,
            fusion_mode='hybrid',
            use_cache=True,
            force_refresh=False
        )
        
        assert result is not None, "Inference returned None"
        assert 'sharpe' in result, "Missing Sharpe ratio in results"
        
        logger.info(f"âœ“ Inference successful (Sharpe: {result['sharpe']:.3f})")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Inference failed: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description='Test cache system')
    parser.add_argument('--symbol', default='AAPL', help='Stock symbol to test')
    parser.add_argument('--skip-training', action='store_true', help='Skip training test (faster)')
    args = parser.parse_args()
    
    symbol = args.symbol.upper()
    
    logger.info(f"\n{'='*70}")
    logger.info(f"CACHE SYSTEM TEST SUITE - {symbol}")
    logger.info(f"{'='*70}\n")
    
    results = {}
    
    # Run tests
    results['cache_creation'] = test_cache_creation(symbol)
    results['cache_invalidation'] = test_cache_invalidation(symbol)
    results['feature_consistency'] = test_feature_consistency(symbol)
    
    if not args.skip_training:
        results['training'] = test_training_with_cache(symbol)
        results['inference'] = test_inference_with_cache(symbol)
    
    # Summary
    logger.info("\n" + "="*70)
    logger.info("TEST SUMMARY")
    logger.info("="*70)
    
    for test_name, passed in results.items():
        status = "âœ“ PASS" if passed else "âŒ FAIL"
        logger.info(f"{status} - {test_name}")
    
    all_passed = all(results.values())
    
    if all_passed:
        logger.info("\nðŸŽ‰ All tests passed!")
        return 0
    else:
        logger.info("\nâŒ Some tests failed")
        return 1


if __name__ == '__main__':
    sys.exit(main())
